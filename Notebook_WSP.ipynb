{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Nohalyan/Projetppchem/blob/Lucas1/Notebook_WSP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9we0xsQKCIis"
   },
   "source": [
    "#Water Solubility Predisction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VBM8XdnDHfh"
   },
   "source": [
    "## 1.1 Import Relevant Modules and Libraries\n",
    "\n",
    "Let's first start by importing relevant modules and libraries needed for this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GFhpnlDlEGpP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib in c:\\users\\venan\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\venan\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\venan\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: rdkit in c:\\users\\venan\\anaconda3\\lib\\site-packages (2022.9.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\venan\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\venan\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.3.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
      "Collecting lazypredict\n",
      "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\venan\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\venan\\anaconda3\\lib\\site-packages (from rdkit) (10.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\venan\\anaconda3\\lib\\site-packages (from lazypredict) (8.1.7)\n",
      "Collecting xgboost (from lazypredict)\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\venan\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\venan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading lightgbm-4.3.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.3 MB 812.7 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.2/1.3 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 5.6 MB/s eta 0:00:00\n",
      "Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
      "Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/99.8 MB 31.4 MB/s eta 0:00:04\n",
      "    --------------------------------------- 1.8/99.8 MB 23.1 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 2.6/99.8 MB 20.7 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 3.5/99.8 MB 18.7 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 4.3/99.8 MB 18.3 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 5.0/99.8 MB 17.9 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 5.8/99.8 MB 17.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 6.6/99.8 MB 17.5 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 7.2/99.8 MB 17.0 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 8.0/99.8 MB 17.0 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 8.7/99.8 MB 16.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 9.4/99.8 MB 16.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 10.2/99.8 MB 16.3 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 11.1/99.8 MB 15.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 11.9/99.8 MB 15.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 12.6/99.8 MB 15.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 13.6/99.8 MB 15.2 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 14.3/99.8 MB 15.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 15.4/99.8 MB 15.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 16.4/99.8 MB 15.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 17.2/99.8 MB 15.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 18.0/99.8 MB 16.0 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 18.9/99.8 MB 16.0 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 19.6/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 20.7/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 21.3/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 22.4/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 23.5/99.8 MB 16.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 24.2/99.8 MB 16.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 25.0/99.8 MB 16.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 25.7/99.8 MB 16.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 26.4/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 27.1/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 27.9/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 28.7/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 29.4/99.8 MB 16.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 30.2/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 30.9/99.8 MB 16.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 31.6/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 31.6/99.8 MB 16.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 32.6/99.8 MB 14.9 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 33.7/99.8 MB 14.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 34.6/99.8 MB 14.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 35.3/99.8 MB 14.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 36.3/99.8 MB 14.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 37.3/99.8 MB 14.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 38.3/99.8 MB 14.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 39.2/99.8 MB 14.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 39.8/99.8 MB 14.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 40.7/99.8 MB 14.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 41.5/99.8 MB 14.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 42.2/99.8 MB 16.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 43.1/99.8 MB 16.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 16.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 44.5/99.8 MB 16.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 45.2/99.8 MB 16.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 46.0/99.8 MB 16.4 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 46.7/99.8 MB 16.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 47.5/99.8 MB 16.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 48.6/99.8 MB 16.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 49.2/99.8 MB 16.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 50.0/99.8 MB 15.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 50.9/99.8 MB 16.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 51.5/99.8 MB 15.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 52.7/99.8 MB 15.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 53.2/99.8 MB 15.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 53.9/99.8 MB 15.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 54.6/99.8 MB 15.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 55.3/99.8 MB 15.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 55.9/99.8 MB 15.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 56.6/99.8 MB 15.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 57.2/99.8 MB 14.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 58.0/99.8 MB 14.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 59.0/99.8 MB 15.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 59.7/99.8 MB 15.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 60.4/99.8 MB 15.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 61.3/99.8 MB 14.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 62.0/99.8 MB 15.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 62.8/99.8 MB 14.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 63.5/99.8 MB 14.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 64.6/99.8 MB 15.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 65.6/99.8 MB 15.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 66.4/99.8 MB 15.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 67.1/99.8 MB 15.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 68.2/99.8 MB 15.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 68.8/99.8 MB 15.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 69.6/99.8 MB 15.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 70.4/99.8 MB 15.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 71.1/99.8 MB 16.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 71.9/99.8 MB 16.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 72.6/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 73.4/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 74.2/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 75.2/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 76.2/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 77.0/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 77.8/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 78.5/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 79.3/99.8 MB 16.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.0/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.8/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 81.9/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 82.8/99.8 MB 16.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.2/99.8 MB 16.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.2/99.8 MB 16.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.2/99.8 MB 16.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.4/99.8 MB 14.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 87.0/99.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 87.7/99.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 88.4/99.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 89.5/99.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 90.2/99.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 91.4/99.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 92.1/99.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 92.9/99.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.6/99.8 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.3/99.8 MB 19.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.4/99.8 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.0/99.8 MB 17.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.7/99.8 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.5/99.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.2/99.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.0/99.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.8/99.8 MB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost, lightgbm, lazypredict\n",
      "Successfully installed lazypredict-0.2.12 lightgbm-4.3.0 xgboost-2.0.3\n"
     ]
    }
   ],
   "source": [
    "# Install all libraries\n",
    "!pip install pathlib numpy pandas rdkit matplotlib scikit-learn lightgbm lazypredict tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFhpnlDlEGpP"
   },
   "source": [
    "Importation of the differents packages function to names for their following using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xDEhgWFsCC0T"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# lazypredict helps to train 42 ML models with a single line of code ans find the best ML models\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HeTUCurHCnk"
   },
   "source": [
    "# 2.1 Let's get the Solubility Data\n",
    "\n",
    "First, we will get solubility data from gashawmg (source: https://github.com/gashawmg) and perform exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XiN0eigVHNgz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Nohalyan/Projetppchem/main/Data_Solubility.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNh8CNiSb3c8"
   },
   "source": [
    "Let's open the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NYPnESo9bPPp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: C:\\Users\\venan\\github\\Projetppchem\n",
      "The file does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Create a Path object for the current directory, in our case /content/\n",
    "current_directory = Path.cwd()\n",
    "print(\"Current Directory:\", current_directory.resolve())\n",
    "\n",
    "file_path = current_directory / \"Data_Solubility.csv\"\n",
    "\n",
    "# Reading the contents of the file and check that the file exists\n",
    "if file_path.exists():\n",
    "    with file_path.open(\"r\") as file:\n",
    "        content = file.read()\n",
    "#       print(content)\n",
    "else:\n",
    "    print(\"The file does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhmAA3fHh1kK"
   },
   "source": [
    "The file use semicicolon as delimiter, so let's open the file and use semicicolon as delimiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3cJRK4pwWNwn"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/Data_Solubility.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# open a file containing descriptors and yield\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_solubility \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/Data_Solubility.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Data_Solubility.csv'"
     ]
    }
   ],
   "source": [
    "# open a file containing descriptors and yield\n",
    "data_solubility = pd.read_csv(\"/content/Data_Solubility.csv\", delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEJ2RcUqhktQ"
   },
   "source": [
    "Check the data see if it is what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2egFBYgjf17S"
   },
   "outputs": [],
   "source": [
    "data_solubility.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9zwa1_mhblV"
   },
   "outputs": [],
   "source": [
    "data_solubility.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFo14mE9hvdt"
   },
   "source": [
    "Looks nice to me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsPHJbgfinOQ"
   },
   "source": [
    "# 2.2 Data Cleaning\n",
    "\n",
    "## 2.2.1 Remove NaN or null values\n",
    "\n",
    "We wil start by removing non-numerical values and valeurs that are null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wnyNXZkicm3"
   },
   "outputs": [],
   "source": [
    "data_solubility.SMILES.isnull().sum()\n",
    "data_solubility.dropna(inplace=True)\n",
    "data_solubility.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC27TZqri8CG"
   },
   "source": [
    "As we can see, the shape is still the same, the data has already been cleaned of non-numerical and null values.\n",
    "##2.2.2 Remove outliers\n",
    "\n",
    "Then, we will remove outliers from the data. Using a boxplot, we can easely visualize outliers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuD5t60TnrX5"
   },
   "outputs": [],
   "source": [
    "sn.set_theme()\n",
    "sn.displot(data=data_solubility, x=\"logS\", binwidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSFjaKMyqCV-"
   },
   "source": [
    "Let's filter compounds that follow as close as normal distribution, let's say between -7.5 and 1.7:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ht-zgHIvqIJ6"
   },
   "outputs": [],
   "source": [
    "new_data_solubility = data_solubility[data_solubility.logS.apply(lambda x: x > -7.5 and x < 1.7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHgDSAQ8rWZr"
   },
   "source": [
    "Let's generate an histogram to see the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wjGJ5AGrW6B"
   },
   "outputs": [],
   "source": [
    "sn.displot(data=new_data_solubility, x='logS', binwidth=1,kde=True)\n",
    "new_data_solubility.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDjkwQEwlcMz"
   },
   "source": [
    "##2.2.3 Remove Duplicates\n",
    "\n",
    "Then remove duplicate by generating canonical SMILES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KydIsfXejaJu"
   },
   "outputs": [],
   "source": [
    "# generate a canonical SMILES function\n",
    "def canonical_SMILES(smiles):\n",
    "    canon_smls = [Chem.CanonSmiles(smls) for smls in smiles]\n",
    "    return canon_smls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MDLZcMzjWtP"
   },
   "outputs": [],
   "source": [
    "# Generate canonical Smiles using the function\n",
    "canon_smiles = canonical_SMILES(new_data_solubility.SMILES)\n",
    "\n",
    "# Replace SMILES column with canonical SMILES\n",
    "new_data_solubility[\"SMILES\"] = canon_smiles\n",
    "\n",
    "# Create a list for duplicate smiles\n",
    "duplicate_smiles = new_data_solubility[new_data_solubility['SMILES'].duplicated()]['SMILES'].values\n",
    "len(duplicate_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7qNi08VkYMz"
   },
   "source": [
    "As we can see, their are 6 duplicates, so we have to filter them and we can also sort them for better reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P73Yy1K7kXzV"
   },
   "outputs": [],
   "source": [
    "new_data_solubility[new_data_solubility['SMILES'].isin(duplicate_smiles)].sort_values(by=['SMILES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRSA4J581bCQ"
   },
   "source": [
    "Let's drop rows that contain duplicate SMILES and keep the first structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jexwvrgv1lSe"
   },
   "outputs": [],
   "source": [
    "data_solubility_cleaned = new_data_solubility.drop_duplicates(subset=['SMILES'], keep='first')\n",
    "data_solubility_cleaned.shape\n",
    "data_solubility_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXSd0vY02KUS"
   },
   "source": [
    "## 2.2.4 Filter training data\n",
    "\n",
    "Now that we have a dataset, let's prepapare a test stet containing 100 drug-like compounds (source: https://github.com/PatWalters/solubility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AsBNHVfErm_"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Nohalyan/Projetppchem/main/Data_Drug_Like_Solubility.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxxXu8QdElDW"
   },
   "outputs": [],
   "source": [
    "# Create a Path object for the current directory, in our case /content/\n",
    "current_directory_dl = Path.cwd()\n",
    "print(\"Current Directory:\", current_directory.resolve())\n",
    "\n",
    "file_path_dl = current_directory / \"Data_Drug_Like_Solubility.csv\"\n",
    "\n",
    "# Reading the contents of the file and check that the file exists\n",
    "if file_path.exists():\n",
    "    with file_path.open(\"r\") as file:\n",
    "        content = file.read()\n",
    "#        print(content)\n",
    "else:\n",
    "    print(\"The file does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uz_TeGiFRWj"
   },
   "outputs": [],
   "source": [
    "data_dl = pd.read_csv(\"/content/Data_Drug_Like_Solubility.csv\", delimiter=';')\n",
    "data_dl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2U7jAFxBGt4i"
   },
   "outputs": [],
   "source": [
    "data_dl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLHQ1dgrHL-m"
   },
   "outputs": [],
   "source": [
    "# Generate canonical Smiles\n",
    "canon_smiles = canonical_SMILES(data_dl.SMILES)\n",
    "\n",
    "# Replace SMILES column wit Canonical SMILES\n",
    "data_dl[\"SMILES\"] = canon_smiles\n",
    "\n",
    "# Create a list for duplicate smiles\n",
    "duplicate_data_dl_smiles = data_dl[data_dl['SMILES'].duplicated()]['SMILES'].values\n",
    "len(duplicate_data_dl_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akgVYCbtHqwk"
   },
   "outputs": [],
   "source": [
    "# Molecules used in training and test of the model\n",
    "data_dl_SMILES = data_dl.SMILES.values\n",
    "\n",
    "# Filter molecules that are not present in the test set\n",
    "data_cleaned_final = data_solubility_cleaned[~data_solubility_cleaned['SMILES'].isin(data_dl_SMILES)]\n",
    "print(f'Compounds present in training set:{len(data_solubility_cleaned) - len(data_cleaned_final)}')\n",
    "data_cleaned_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEoNO8CMLVqZ"
   },
   "outputs": [],
   "source": [
    "data_dl= data_dl[data_dl['LogS exp (mol/L)'].apply(lambda x: x > -7.5 and x < 1.7)]\n",
    "data_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwdeeEJMLtxc"
   },
   "source": [
    "# 3. Calculation of RDkit Molecular Descriptors, which are molecular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiwjxW8iL39L"
   },
   "outputs": [],
   "source": [
    "def RDkit_descriptors(smiles):\n",
    "    mols = [Chem.MolFromSmiles(i) for i in smiles]\n",
    "    calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0]\n",
    "                                    for x in Descriptors._descList])\n",
    "    desc_names = calc.GetDescriptorNames()\n",
    "\n",
    "    Mol_descriptors =[]\n",
    "    for mol in tqdm(mols):\n",
    "        # add hydrogens to molecules\n",
    "        mol=Chem.AddHs(mol)\n",
    "        # Calculate all 200 descriptors for each molecule\n",
    "        descriptors = calc.CalcDescriptors(mol)\n",
    "        Mol_descriptors.append(descriptors)\n",
    "    return Mol_descriptors,desc_names\n",
    "\n",
    "# Function call\n",
    "Mol_descriptors,desc_names = RDkit_descriptors(data_cleaned_final['SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1v7UGPE6OLKA"
   },
   "outputs": [],
   "source": [
    "df_descriptors = pd.DataFrame(Mol_descriptors, columns=desc_names)\n",
    "df_descriptors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLerraWYQhCX"
   },
   "outputs": [],
   "source": [
    "df_descriptors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZFGKSecQqsq"
   },
   "source": [
    "# 4. Split the chemicals for training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtfxtv4bQ1JW"
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(df_descriptors, data_cleaned_final.logS, test_size=0.1,random_state=42)\n",
    "\n",
    "#Standardization of the features\n",
    "\n",
    "custom_scaler = StandardScaler()\n",
    "custom_scaler.fit(x_train)\n",
    "x_train_scaled = custom_scaler.transform(x_train)\n",
    "x_valid_scaled = custom_scaler.transform(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObN4zyriRhtz"
   },
   "source": [
    "#5. Select Machine Learning Models\n",
    "\n",
    "Let's selct the best ML models for that. To do that, we will use the lazypredict librarie, in particular the LazyRegressor function to test 42 ML models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVj91hWoRkr-"
   },
   "outputs": [],
   "source": [
    "lregs = LazyRegressor(verbose=0,ignore_warnings=True, custom_metric=None,random_state=42)\n",
    "models, prediction_tests = lregs.fit(x_train_scaled, x_valid_scaled, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djOzUCFpWF-z"
   },
   "outputs": [],
   "source": [
    "#The top three models\n",
    "prediction_tests[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHz1RVk6WlW5"
   },
   "source": [
    "#6. Fine-tuning of LGBMRegressor\n",
    "\n",
    "We decided to take the LGBMRegressor model because the results generated by this model are comparable to the ExtraTreesRegressor model, but takes a lot less time than the extra-trees model.Let's performs a grid search using GridSearchCV from scikit-learn to find the best hyperparameters for a LightGBM regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4C5cOJ4hXG3M"
   },
   "outputs": [],
   "source": [
    "#params = {'max_depth' : list(range(2, 32, 8)),\n",
    "#          'n_estimators' : list(range(1, 1000, 100)),\n",
    "#          'learning_rate' : list(np.arange(0.01, 1.02, 0.25))}\n",
    "#\n",
    "#grid_search = GridSearchCV(LGBMRegressor(random_state = 42),\n",
    "#                            param_grid=params, cv=5, verbose=1)\n",
    "#\n",
    "#grid_search.fit(x_train, y_train)\n",
    "#\n",
    "#print(\"Optimized parameters for a LightGBM regressor can be: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELET-iZOcmPx"
   },
   "source": [
    "We obtained:\n",
    "* learning_rate: 0.01\n",
    "* max_depth: 26\n",
    "* n_estmitors: 901\n",
    "\n",
    "Let's optimize again with new ranges for liste max_depth, n_estmitors and learning_rate to obtain even better parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kwe8dQKHNTzt"
   },
   "outputs": [],
   "source": [
    "#params_bst = {\"max_depth\" : list(range(20, 36, 4)),\n",
    "#         \"n_estimators\" : list(range(850, 1200, 50)),\n",
    "#         \"learning_rate\" : list(np.arange(0.01, 0.05, 0.01))}\n",
    "#\n",
    "#grid_search_bst = GridSearchCV(LGBMRegressor(random_state = 42),\n",
    "#                                  param_grid=params_bst, cv=3, verbose=1)\n",
    "#\n",
    "#grid_search_bst.fit(x_train, y_train)\n",
    "#print(\"The best parameters are: \", grid_search_bst.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfuMUiEYAu01"
   },
   "source": [
    "#7. LGBMRegressor model for training and test data\n",
    "Let's test our model ont the training and test set with the best parameters found with the fine tunning:\n",
    "* learning_rate: 0.04\n",
    "* max_depth: 24\n",
    "* n_estmitors: 1150\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV5YnDz4BOhz"
   },
   "outputs": [],
   "source": [
    "model = LGBMRegressor(n_estimators = 1150,\n",
    "                      max_depth = 24,\n",
    "                      learning_rate = 0.04,\n",
    "                      random_state= 42)\n",
    "\n",
    "model.fit(x_train_scaled,y_train)\n",
    "y_preds = model.predict(x_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYAhA4_1ChMy"
   },
   "outputs": [],
   "source": [
    "# A plotting function\n",
    "def plot_data(actual, predicted, title):\n",
    "\n",
    "# model performance using RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# R^2 (coefficient of determination) :\n",
    "    R2 =r2_score(actual, predicted)\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot the figure\n",
    "    sn.regplot(x=predicted , y=actual,line_kws={\"lw\":2,\n",
    "                                                \"ls\":\"--\",\n",
    "                                                \"color\":\"red\",\n",
    "                                                \"alpha\":0.7})\n",
    "    plt.title(title, color=\"red\")\n",
    "    plt.xlabel(\"Predicted logS(mol/L)\",\n",
    "               color=\"blue\")\n",
    "    plt.xlim(-8,1)\n",
    "    plt.ylabel(\"Experimental logS(mol/L)\",\n",
    "               color =\"blue\")\n",
    "\n",
    "\n",
    "    plt.grid(alpha=0.3)\n",
    "    R2 = mpatches.Patch(label=\"R2={:04.2f}\".format(R2))\n",
    "    rmse = mpatches.Patch(label=\"RMSE={:04.2f}\".format(rmse))\n",
    "    plt.legend(handles=[R2, rmse])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLXafM4bEpJg"
   },
   "source": [
    "Let's plot the predicted logS of the validation set and see if our model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtO2vsUUEnMh"
   },
   "outputs": [],
   "source": [
    "sn.set_theme(style=\"whitegrid\")\n",
    "plot_data(y_valid,y_preds,\"Validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzzUsNv0En7a"
   },
   "outputs": [],
   "source": [
    "#Calculate molecular descriptors for the test data or 98 compounds\n",
    "\n",
    "Mol_descriptors_test , desc_names_test = RDkit_descriptors(data_dl[\"SMILES\"])\n",
    "data_dl_descriptors = pd.DataFrame(Mol_descriptors_test,columns=desc_names_test)\n",
    "\n",
    "# Standard scaler - transform\n",
    "x_scaled_test = custom_scaler.transform(data_dl_descriptors)\n",
    "\n",
    "# Predict solubility of the test data\n",
    "y_test_preds = model.predict(x_scaled_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_U34X3eEju"
   },
   "outputs": [],
   "source": [
    "# Plotting testing set\n",
    "sn.set_theme(style=\"whitegrid\")\n",
    "plot_data(data_dl[\"LogS exp (mol/L)\"], y_test_preds,\n",
    "           \"Test data: Drug-like Molecules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG-nNTVGJGQJ"
   },
   "source": [
    "#8. Saving of the trained model and standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBUk2hL7JdwC"
   },
   "outputs": [],
   "source": [
    "with open(\"model_WSP.pkl\",\"wb\") as f:\n",
    "    pickle.dump(model,f)\n",
    "\n",
    "with open(\"scaler_WSP.pkl\",\"wb\") as f:\n",
    "    pickle.dump(custom_scaler,f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNsL6ZKH9JH9vAaJCzvPRsq",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
