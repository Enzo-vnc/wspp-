{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Nohalyan/Projetppchem/blob/main/Notebook_WSP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9we0xsQKCIis"
   },
   "source": [
    "#Water Solubility Predisction\n",
    "\n",
    "This notebook is the core of our Water Solubility Prediction project. This is where we collected, sorted and cleaned the data, as well as selecting, testing and training our prediction models.\n",
    "\n",
    "This notebook is divided into sections:\n",
    "1. Modules and Libraries\n",
    "2. Solubility Data *(optional)*\n",
    "3. Calculation of RDkit Molecular Descriptors *(optional)*\n",
    "4. Training and Validation set\n",
    "5. Select Machine Learning Models\n",
    "6. Fine-tuning\n",
    "7. Models for training and test data\n",
    "\n",
    "Sections 2 and 3 are optional, as sorted and cleaned data can be retrieved directly in section 4.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VBM8XdnDHfh"
   },
   "source": [
    "## 1 Import Relevant Modules and Libraries\n",
    "\n",
    "We will start by importing the modules and libraries essential for this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFhpnlDlEGpP"
   },
   "outputs": [],
   "source": [
    "# Install all libraries used in this project\n",
    "!pip install pathlib numpy pandas rdkit matplotlib scikit-learn lightgbm lazypredict tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xDEhgWFsCC0T"
   },
   "outputs": [],
   "source": [
    "# Install all modules used in this project\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcKv90-s7Y00"
   },
   "source": [
    "# 2. Solubility Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HeTUCurHCnk"
   },
   "source": [
    "## 2.1 Let's get the Solubility Data\n",
    "\n",
    "Firstly, we will acquire solubility data from gashawmg's repository, accessible at https://github.com/gashawmg. Subsequently, we will conduct an exploratory data analysis on the acquired dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiN0eigVHNgz"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Nohalyan/Projetppchem/main/Data/Data_Solubility.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNh8CNiSb3c8"
   },
   "source": [
    "Let's open the file ans confirm that it exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYPnESo9bPPp"
   },
   "outputs": [],
   "source": [
    "# Create a Path object for the current directory, in our case /content/\n",
    "current_directory = Path.cwd()\n",
    "print(\"Current Directory:\", current_directory.resolve())\n",
    "\n",
    "file_path = current_directory / \"Data_Solubility.csv\"\n",
    "\n",
    "# Reading the contents of the file and check that the file exists\n",
    "if file_path.exists():\n",
    "    with file_path.open(\"r\") as file:\n",
    "        content = file.read()\n",
    "        print(content)\n",
    "else:\n",
    "    print(\"The file does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhmAA3fHh1kK"
   },
   "source": [
    "The file use semicicolon as delimiter, so let's open the file and use semicicolon as delimiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3cJRK4pwWNwn"
   },
   "outputs": [],
   "source": [
    "# Open a file containing descriptors and yield\n",
    "data_solubility = pd.read_csv(\"/content/Data_Solubility.csv\", delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEJ2RcUqhktQ"
   },
   "source": [
    "Now we will check the data see if it is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2egFBYgjf17S"
   },
   "outputs": [],
   "source": [
    "data_solubility.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9zwa1_mhblV"
   },
   "outputs": [],
   "source": [
    "data_solubility.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsPHJbgfinOQ"
   },
   "source": [
    "## 2.2 Data Cleaning\n",
    "\n",
    "Now that we've got our data set, we need to clean it up. To do this, we're going to remove non-numericals (NaN) and null values. We'll also sort outliers and make SMILES canonical, so as to duplicate them and obtain a ready-to-use data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EmTTwOx74Mw"
   },
   "source": [
    "### 2.2.1 Remove NaN or null values\n",
    "\n",
    "We wil start by removing non-numerical values and valeurs that are null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wnyNXZkicm3",
    "outputId": "01d4ed53-5e97-4b9e-88b3-a89c5b70b239"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9943, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_solubility.SMILES.isnull().sum()\n",
    "data_solubility.dropna(inplace=True)\n",
    "data_solubility.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC27TZqri8CG"
   },
   "source": [
    "As we can see, the shape is still the same, the data has already been cleaned of non-numerical and null values.\n",
    "\n",
    "###2.2.2 Remove outliers\n",
    "\n",
    "Then, we will remove outliers from the data. Using a boxplot, we can easely visualize outliers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuD5t60TnrX5"
   },
   "outputs": [],
   "source": [
    "sn.set_theme()\n",
    "sn.displot(data=data_solubility, x=\"logS\", binwidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSFjaKMyqCV-"
   },
   "source": [
    "We will then filter compounds that follow as close as normal distribution. We consider values between -7.5 and 1.7 as following a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ht-zgHIvqIJ6"
   },
   "outputs": [],
   "source": [
    "new_data_solubility = data_solubility[data_solubility.logS.apply(lambda x: x > -7.5 and x < 1.7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHgDSAQ8rWZr"
   },
   "source": [
    "We will generate a histogram to visualize the distribution of the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wjGJ5AGrW6B"
   },
   "outputs": [],
   "source": [
    "sn.displot(data=new_data_solubility, x='logS', binwidth=1,kde=True)\n",
    "new_data_solubility.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDjkwQEwlcMz"
   },
   "source": [
    "### 2.2.3 Remove Duplicates\n",
    "\n",
    "Then, we will remove duplicate by generating canonical SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KydIsfXejaJu"
   },
   "outputs": [],
   "source": [
    "# Generate a canonical SMILES function\n",
    "def canonical_SMILES(smiles):\n",
    "    canon_smls = [Chem.CanonSmiles(smls) for smls in smiles]\n",
    "    return canon_smls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MDLZcMzjWtP"
   },
   "outputs": [],
   "source": [
    "# Generate canonical Smiles using the function\n",
    "canon_smiles = canonical_SMILES(new_data_solubility.SMILES)\n",
    "\n",
    "# Replace SMILES column with canonical SMILES\n",
    "new_data_solubility[\"SMILES\"] = canon_smiles\n",
    "\n",
    "# Create a list for duplicate smiles\n",
    "duplicate_smiles = new_data_solubility[new_data_solubility['SMILES'].duplicated()]['SMILES'].values\n",
    "len(duplicate_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7qNi08VkYMz"
   },
   "source": [
    "Upon analysis,we found out that there are six duplicate entries. These duplicates will be filtered out and the dataset will be sorted for improved readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P73Yy1K7kXzV"
   },
   "outputs": [],
   "source": [
    "new_data_solubility[new_data_solubility['SMILES'].isin(duplicate_smiles)].sort_values(by=['SMILES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRSA4J581bCQ"
   },
   "source": [
    "We removed rows containing duplicate SMILES, retaining the first occurrence of each structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jexwvrgv1lSe"
   },
   "outputs": [],
   "source": [
    "data_solubility_cleaned = new_data_solubility.drop_duplicates(subset=['SMILES'], keep='first')\n",
    "data_solubility_cleaned.shape\n",
    "data_solubility_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXSd0vY02KUS"
   },
   "source": [
    "### 2.2.4 Filter training data\n",
    "With the dataset in hand, our next step involves assembling a test set comprising 100 drug-like compounds. These compounds were sourced from https://github.com/PatWalters/solubility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AsBNHVfErm_"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Nohalyan/Projetppchem/main/Data/Data_Drug_Like_Solubility.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxxXu8QdElDW"
   },
   "outputs": [],
   "source": [
    "# Create a Path object for the current directory, in our case /content/\n",
    "current_directory_dl = Path.cwd()\n",
    "print(\"Current Directory:\", current_directory.resolve())\n",
    "\n",
    "file_path_dl = current_directory / \"Data_Drug_Like_Solubility.csv\"\n",
    "\n",
    "# Reading the contents of the file and check that the file exists\n",
    "if file_path.exists():\n",
    "    with file_path.open(\"r\") as file:\n",
    "        content = file.read()\n",
    "#        print(content)\n",
    "else:\n",
    "    print(\"The file does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uz_TeGiFRWj"
   },
   "outputs": [],
   "source": [
    "data_dl = pd.read_csv(\"/content/Data_Drug_Like_Solubility.csv\", delimiter=';')\n",
    "data_dl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2U7jAFxBGt4i"
   },
   "outputs": [],
   "source": [
    "data_dl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLHQ1dgrHL-m"
   },
   "outputs": [],
   "source": [
    "# Generate canonical Smiles\n",
    "canon_smiles = canonical_SMILES(data_dl.SMILES)\n",
    "\n",
    "# Replace SMILES column wit Canonical SMILES\n",
    "data_dl[\"SMILES\"] = canon_smiles\n",
    "\n",
    "# Create a list for duplicate smiles\n",
    "duplicate_data_dl_smiles = data_dl[data_dl['SMILES'].duplicated()]['SMILES'].values\n",
    "len(duplicate_data_dl_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgDAlHuCF0Vd"
   },
   "source": [
    "Now we need to check that there are no duplicates between the data set and the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akgVYCbtHqwk"
   },
   "outputs": [],
   "source": [
    "# Molecules used in training and test of the model\n",
    "data_dl_SMILES = data_dl.SMILES.values\n",
    "\n",
    "# Filter molecules that are not present in the test set\n",
    "data_cleaned_final = data_solubility_cleaned[~data_solubility_cleaned['SMILES'].isin(data_dl_SMILES)]\n",
    "print(f'Compounds present in training set:{len(data_solubility_cleaned) - len(data_cleaned_final)}')\n",
    "data_cleaned_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Rql4HJ_sGOia"
   },
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file\n",
    "data_cleaned_final.to_csv('Data_Cleaned_Final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEoNO8CMLVqZ"
   },
   "outputs": [],
   "source": [
    "data_dl= data_dl[data_dl['LogS exp (mol/L)'].apply(lambda x: x > -7.5 and x < 1.7)]\n",
    "data_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwdeeEJMLtxc"
   },
   "source": [
    "# 3. Calculation of RDkit Molecular Descriptors\n",
    "\n",
    "Calculation of RDkit Molecular Descriptors, which are molecular features for the data set and the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiwjxW8iL39L"
   },
   "outputs": [],
   "source": [
    "def RDkit_descriptors(smiles):\n",
    "    mols = [Chem.MolFromSmiles(i) for i in smiles]\n",
    "    calc = MoleculeDescriptors.MolecularDescriptorCalculator([x[0]\n",
    "                                    for x in Descriptors._descList])\n",
    "    desc_names = calc.GetDescriptorNames()\n",
    "\n",
    "    Mol_descriptors =[]\n",
    "    for mol in tqdm(mols):\n",
    "        # add hydrogens to molecules\n",
    "        mol=Chem.AddHs(mol)\n",
    "        # Calculate all 200 descriptors for each molecule\n",
    "        descriptors = calc.CalcDescriptors(mol)\n",
    "        Mol_descriptors.append(descriptors)\n",
    "    return Mol_descriptors,desc_names\n",
    "\n",
    "# Function call\n",
    "Mol_descriptors,desc_names = RDkit_descriptors(data_cleaned_final['SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1v7UGPE6OLKA"
   },
   "outputs": [],
   "source": [
    "df_descriptors = pd.DataFrame(Mol_descriptors, columns=desc_names)\n",
    "df_descriptors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBYTOA2jGUff"
   },
   "outputs": [],
   "source": [
    "# Calculate molecular descriptors for the test data or 98 compounds\n",
    "Mol_descriptors_test , desc_names_test = RDkit_descriptors(data_dl[\"SMILES\"])\n",
    "data_dl_descriptors = pd.DataFrame(Mol_descriptors_test,columns=desc_names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_fps8kamGUWI"
   },
   "outputs": [],
   "source": [
    "# Save the dataf to .csv files\n",
    "df_descriptors.to_csv('Data_Solubility_descriptor.csv', index=False)\n",
    "data_dl_descriptors.to_csv('Data_Solubility_Drug_Like_descriptor.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZFGKSecQqsq"
   },
   "source": [
    "# 4. Training and Validation set\n",
    "\n",
    "In this part, we will separate the data from the data set into 2 sets: the training set to train the model and the validation set to provide an evaluation of the model on the training dataset to adjust the hyperparameters.\n",
    "\n",
    "If you have run sections 2. and 3., you can skip to section 4.2, otherwise you must run section 4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0FnHdNK-UjL"
   },
   "source": [
    "##4.1 Acquisition of Sorted, Cleaned Data Sets and RDkit Molecular Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYAMHTM8-BkU"
   },
   "outputs": [],
   "source": [
    "#Download cleaned and sorted data (Data_Cleaned_Final.csv) with RDkit molecular descriptors (Data_Solubility_descriptor.csv) for training and validation set\n",
    "!wget https://raw.githubusercontent.com/Nohalyan/Projetppchem/main/Data/Data_Cleaned_Final.csv\n",
    "!wget https://raw.githubusercontent.com/Nohalyan/Projetppchem/main/Data/Data_Solubility_descriptor.csv\n",
    "\n",
    "#Download cleaned and sorted data (Data_Drug_Like_Solubility.csv) with RDkit molecular descriptors (Data_Solubility_Drug_Like_descriptor.csv) for test set\n",
    "!wget https://raw.githubusercontent.com/Nohalyan/Projetppchem/main/Data/Data_Drug_Like_Solubility.csv\n",
    "!wget https://raw.githubusercontent.com/Nohalyan/Projetppchem/main/Data/Data_Solubility_Drug_Like_descriptor.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0DNJaLB_Z92"
   },
   "outputs": [],
   "source": [
    "#To verify that the file was correctly downloaded\n",
    "# Create a Path object for the current directory, in our case /content/\n",
    "current_directory = Path.cwd()\n",
    "print(\"Current Directory:\", current_directory.resolve())\n",
    "\n",
    "file_path = current_directory / \"Data_Solubility_Drug_Like_descriptor.csv\" #Copy and paste the file name\n",
    "\n",
    "# Reading the contents of the file and check that the file exists\n",
    "if file_path.exists():\n",
    "    with file_path.open(\"r\") as file:\n",
    "        content = file.read()\n",
    "        print(content)\n",
    "else:\n",
    "    print(\"The file does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MPM1lkSP-3n8"
   },
   "outputs": [],
   "source": [
    "#Variable definitions, if not defined in sections 2. and 3.\n",
    "data_dl = pd.read_csv(\"/content/Data_Drug_Like_Solubility.csv\", delimiter=';')\n",
    "data_dl= data_dl[data_dl['LogS exp (mol/L)'].apply(lambda x: x > -7.5 and x < 1.7)]\n",
    "df_descriptors = pd.read_csv(\"/content/Data_Solubility_descriptor.csv\", delimiter=';')\n",
    "data_cleaned_final = pd.read_csv(\"/content/Data_Cleaned_Final.csv\", delimiter=',')\n",
    "data_dl_descriptors = pd.read_csv(\"/content/Data_Solubility_Drug_Like_descriptor.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_Jh0XofAngT"
   },
   "source": [
    "## 4.2  Split the chemicals for training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xtfxtv4bQ1JW"
   },
   "outputs": [],
   "source": [
    "#Variable definitions for training and set validation\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(df_descriptors, data_cleaned_final.logS, test_size=0.1,random_state=42)\n",
    "\n",
    "#Standardization of the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObN4zyriRhtz"
   },
   "source": [
    "#5. Select Machine Learning Models\n",
    "\n",
    "Let's identify the optimal machine learning models for our water solubility predisction. To accomplish this, we will utilize the \"lazypredict\" library, specifically leveraging the \"LazyRegressor\" function to evaluate a selection of 42 machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVj91hWoRkr-"
   },
   "outputs": [],
   "source": [
    "lregs = LazyRegressor(verbose=0,ignore_warnings=True, custom_metric=None,random_state=42)\n",
    "models, prediction_tests = lregs.fit(x_train_scaled, x_valid_scaled, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djOzUCFpWF-z"
   },
   "outputs": [],
   "source": [
    "#The top five models\n",
    "prediction_tests[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHz1RVk6WlW5"
   },
   "source": [
    "# 6. Fine-tuning\n",
    "\n",
    "Analyzing the top 5 best models we decided to use 3 models:\n",
    "1. LGBMRegressor,\n",
    "2. XGBRegressor because these are the 2 fastest models, enabling us to obtain results faster than the other models.\n",
    "3. RandomForestRegressor because we were familiar with this modeland we wanted to see what it could do in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0xvrdCh28V5"
   },
   "source": [
    "## 6.1 LGBMRegressor\n",
    "\n",
    "We decided to take the LGBMRegressor model because the results generated by this model are comparable to the ExtraTreesRegressor model, but takes a lot less time than the ExtraTreesRegressor model.Let's performs a grid search using GridSearchCV from scikit-learn to find the best hyperparameters for a LightGBM regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4C5cOJ4hXG3M"
   },
   "outputs": [],
   "source": [
    "params = {'max_depth' : list(range(20, 30, 1)),\n",
    "          'n_estimators' : list(range(1278, 1280, 1)) ,\n",
    "          'learning_rate': list(np.arange(0.04, 0.05, 0.01))}\n",
    "\n",
    "grid_search = GridSearchCV(LGBMRegressor(random_state = 42),\n",
    "                            param_grid=params, cv=5, verbose=1)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(\"Optimized parameters for a LightGBM regressor can be: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELET-iZOcmPx"
   },
   "source": [
    "We obtained:\n",
    "* learning_rate: 0.01\n",
    "* max_depth: 26\n",
    "* n_estmitors: 901\n",
    "\n",
    "Let's optimize again with new ranges for max_depth, n_estmitors and learning_rate to obtain even better parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kwe8dQKHNTzt"
   },
   "outputs": [],
   "source": [
    "#params_bst = {\"max_depth\" : list(range(20, 36, 4)),\n",
    "#         \"n_estimators\" : list(range(850, 1200, 50)),\n",
    "#         \"learning_rate\" : list(np.arange(0.01, 0.05, 0.01))}\n",
    "#\n",
    "#grid_search_bst = GridSearchCV(LGBMRegressor(random_state = 42),\n",
    "#                                  param_grid=params_bst, cv=3, verbose=1)\n",
    "#\n",
    "#grid_search_bst.fit(x_train, y_train)\n",
    "#print(\"The best parameters are: \", grid_search_bst.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-EtroNCMEW2"
   },
   "source": [
    "We obtained:\n",
    "* learning_rate: 0.04\n",
    "* max_depth: 24\n",
    "* n_estmitors: 1151\n",
    "\n",
    "These are the parameters we will use for our standard LGBMRegressor model.\n",
    "\n",
    "We also wanted to see if by making new runs and trying to optimize these three hyperparameters even further, we could get an even better model. We therefore made a total of 6 runs (compared with the 2 runs for standard models) for this model and obtained these parameters:\n",
    "* learning_rate: 0.04\n",
    "* max_depth: 21\n",
    "* n_estmitors: 1279\n",
    "\n",
    "These are the parameters we will use for our studious LGBMRegressor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_Azw2-qZRI7"
   },
   "source": [
    "##6.2 XGB Regressor\n",
    "In order to compare our results obtained for the first model, we have chosen XGB Regressor as the second model for the reasons presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiCxejpQZWW8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming x_train and y_train are already defined\n",
    "\n",
    "# Split data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "\n",
    "# Define the parameter grid\n",
    "max_depths = list(range(2, 32, 8))\n",
    "n_estimators = list(range(1, 201, 20))\n",
    "learning_rates = list(np.arange(0.01, 1.02, 0.25))\n",
    "\n",
    "best_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over parameter combinations\n",
    "for max_depth in max_depths:\n",
    "    for n_estimator in n_estimators:\n",
    "        for learning_rate in learning_rates:\n",
    "            # Create XGBoost regressor with current parameters\n",
    "            xgb_regressor = XGBRegressor(max_depth=max_depth,\n",
    "                                         n_estimators=n_estimator,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         random_state=42)\n",
    "\n",
    "            # Fit the model\n",
    "            xgb_regressor.fit(x_train_scaled, y_train)\n",
    "\n",
    "            # Evaluate the model\n",
    "            score = xgb_regressor.score(x_val_scaled, y_val)  # Using the validation set for evaluation\n",
    "\n",
    "            # Check if current parameters yield better score\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'max_depth': max_depth, 'n_estimators': n_estimator, 'learning_rate': learning_rate}\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Optimized parameters for an XGBoost regressor can be:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5ES7ZsPZh54"
   },
   "source": [
    "We obtained for this first run:\n",
    "- max_depth: 2\n",
    "- n_estimators: 181\n",
    "- learning_rate: 0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0hA410WZqpG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming x_train and y_train are already defined\n",
    "\n",
    "# Split data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data preprocessing\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "\n",
    "# Define the parameter grid\n",
    "max_depths = list(range(2, 6, 1))\n",
    "n_estimators = list(range(100, 700, 50))\n",
    "learning_rates = list(np.arange(0.05, 0.30, 0.05))\n",
    "\n",
    "best_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over parameter combinations\n",
    "for max_depth in max_depths:\n",
    "    for n_estimator in n_estimators:\n",
    "        for learning_rate in learning_rates:\n",
    "            # Create XGBoost regressor with current parameters\n",
    "            xgb_regressor = XGBRegressor(max_depth=max_depth,\n",
    "                                         n_estimators=n_estimator,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         random_state=42)\n",
    "\n",
    "            # Fit the model\n",
    "            xgb_regressor.fit(x_train_scaled, y_train)\n",
    "\n",
    "            # Evaluate the model\n",
    "            score = xgb_regressor.score(x_val_scaled, y_val)  # Using the validation set for evaluation\n",
    "\n",
    "            # Check if current parameters yield better score\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'max_depth': max_depth, 'n_estimators': n_estimator, 'learning_rate': learning_rate}\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Optimized parameters for an XGBoost regressor can be:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3NB61q4ZvZM"
   },
   "source": [
    "\n",
    "We obtained for this second run:\n",
    "\n",
    "- max_depth: 5\n",
    "- n_estimators: 500\n",
    "- learning_rate: 0.1\n",
    "\n",
    "These are the parameters we will use for our standard LGBMRegressor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS5PzOwgiScM"
   },
   "source": [
    "## 6.3 RandomForestRegressor\n",
    "\n",
    "For the third regression model, we opted for the RandomForestRegressor model. However, it presented longer runtimes, allowing us to explore its post-tuning performance and see how it differs from the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpfQVi3kLTA4"
   },
   "outputs": [],
   "source": [
    "# Handle missing values in x_train\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "x_train_imputed = imputer.fit_transform(x_train)\n",
    "\n",
    "# Scale the input data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_imputed)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "paramsRF = {\n",
    "    'max_depth': list(range(2, 32, 16)),\n",
    "    'n_estimators': list(range(1, 1000, 200))\n",
    "}\n",
    "\n",
    "# Perform grid search with RandomForestRegressor\n",
    "grid_searchRF = GridSearchCV(RandomForestRegressor(random_state=42),\n",
    "                             param_grid=paramsRF, cv=5, verbose=1)\n",
    "\n",
    "# Fit the grid search to the scaled data\n",
    "grid_searchRF.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Optimized parameters for a RandomForestRegressor can be: \", grid_searchRF.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1RDwyFdFttU"
   },
   "source": [
    "We acquired those values for the first run:\n",
    "- max_depth = 26\n",
    "- n_estimators = 901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-1f_CDkE6Qu"
   },
   "outputs": [],
   "source": [
    "# Handle missing values in x_train\n",
    "#imputer = SimpleImputer(strategy='mean')\n",
    "#x_train_imputed = imputer.fit_transform(x_train)\n",
    "\n",
    "# Scale the input data\n",
    "#scaler = StandardScaler()\n",
    "#x_train_scaled = scaler.fit_transform(x_train_imputed)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "#paramsRF = {\n",
    "#    'max_depth': list(range(2, 32, 12)),\n",
    "#    'n_estimators': list(range(1, 1000, 150))\n",
    "#}\n",
    "\n",
    "# Perform grid search with RandomForestRegressor\n",
    "#grid_searchRF = GridSearchCV(RandomForestRegressor(random_state=42),\n",
    "#                             param_grid=paramsRF, cv=5, verbose=1)\n",
    "\n",
    "# Fit the grid search to the scaled data\n",
    "#grid_searchRF.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "#print(\"Optimized parameters for a RandomForestRegressor can be: \", grid_searchRF.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDvLKn-kEbc5"
   },
   "source": [
    "We acquired those values for the first run:\n",
    "- max_depth = 18\n",
    "- n_estimators = 801\n",
    "\n",
    "These are the parameters we will use for our studious RandomForestRegressor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgaMm6uzhaz6"
   },
   "source": [
    "# 7. Models for training and test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfuMUiEYAu01"
   },
   "source": [
    "## 7.1 LGBMRegressor\n",
    "Let's test our model ont the training and test set with the best parameters found with the fine tunning:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV5YnDz4BOhz"
   },
   "outputs": [],
   "source": [
    "model = LGBMRegressor(n_estimators = 1279,\n",
    "                      max_depth = 21,\n",
    "                      learning_rate = 0.04,\n",
    "                      random_state= 42)\n",
    "\n",
    "model.fit(x_train_scaled,y_train)\n",
    "y_preds = model.predict(x_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vYAhA4_1ChMy"
   },
   "outputs": [],
   "source": [
    "# A plotting function\n",
    "def plot_data(actual, predicted, title):\n",
    "\n",
    "# model performance using RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# R^2 (coefficient of determination) :\n",
    "    R2 =r2_score(actual, predicted)\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot the figure\n",
    "    sn.regplot(x=predicted , y=actual,line_kws={\"lw\":2,\n",
    "                                                \"ls\":\"--\",\n",
    "                                                \"color\":\"red\",\n",
    "                                                \"alpha\":0.7})\n",
    "    plt.title(title, color=\"red\")\n",
    "    plt.xlabel(\"Predicted logS(mol/L)\",\n",
    "               color=\"blue\")\n",
    "    plt.xlim(-8,1)\n",
    "    plt.ylabel(\"Experimental logS(mol/L)\",\n",
    "               color =\"blue\")\n",
    "\n",
    "    plt.grid(alpha=0.3)\n",
    "    R2 = mpatches.Patch(label=\"R2={:04.2f}\".format(R2))\n",
    "    rmse = mpatches.Patch(label=\"RMSE={:04.2f}\".format(rmse))\n",
    "    plt.legend(handles=[R2, rmse])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLXafM4bEpJg"
   },
   "source": [
    "Let's plot the predicted logS of the validation set and see if our model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtO2vsUUEnMh"
   },
   "outputs": [],
   "source": [
    "sn.set_theme(style=\"whitegrid\")\n",
    "plot_data(y_valid,y_preds,\"Validation data LGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PzzUsNv0En7a",
    "outputId": "aae81c50-edfb-4d6a-e6cc-72b2b04ced11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "# Standard scaler - transform\n",
    "x_scaled_test = scaler.transform(data_dl_descriptors)\n",
    "\n",
    "# Predict solubility of the test data\n",
    "y_test_preds = model.predict(x_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf_U34X3eEju"
   },
   "outputs": [],
   "source": [
    "# Plotting testing set\n",
    "sn.set_theme(style=\"whitegrid\")\n",
    "plot_data(data_dl[\"LogS exp (mol/L)\"], y_test_preds,\n",
    "           \"Test data: Drug-like Molecules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3caSSsgQlgvq"
   },
   "source": [
    "## 7.2 XGB Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6fmlsABklkUk"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model_XGB = XGBRegressor(n_estimators = 500,\n",
    "                         max_depth = 5,\n",
    "                         learning_rate = 0.1,\n",
    "                         random_state = 42)\n",
    "model_XGB.fit(x_train_scaled,y_train)\n",
    "y_preds_XGB = model_XGB.predict(x_valid_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6y9KrGGYm3Mr"
   },
   "outputs": [],
   "source": [
    "# A plotting function\n",
    "def plot_data_XGB(actual, predicted, title):\n",
    "\n",
    "# model performance using RMSE\n",
    "    rmse_XGB = np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# R^2 (coefficient of determination) :\n",
    "    R2_XGB =r2_score(actual, predicted)\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot the figure\n",
    "    sn.regplot(x=predicted , y=actual,line_kws={\"lw\":2,\n",
    "                                                \"ls\":\"--\",\n",
    "                                                \"color\":\"red\",\n",
    "                                                \"alpha\":0.7})\n",
    "    plt.title(title, color=\"red\")\n",
    "    plt.xlabel(\"Predicted logS(mol/L)\",\n",
    "               color=\"blue\")\n",
    "    plt.xlim(-8,1)\n",
    "    plt.ylabel(\"Experimental logS(mol/L)\",\n",
    "               color =\"blue\")\n",
    "\n",
    "\n",
    "    plt.grid(alpha=0.3)\n",
    "    R2_XGB = mpatches.Patch(label=\"R2_XGB={:04.2f}\".format(R2_XGB))\n",
    "    rmse_XGB = mpatches.Patch(label=\"RMSE_XGB={:04.2f}\".format(rmse_XGB))\n",
    "    plt.legend(handles=[R2_XGB, rmse_XGB])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVS679LeR0hA"
   },
   "source": [
    "Let's plot the predicted logS of the validation set and see if our model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUZ-QwQ1nTFN"
   },
   "outputs": [],
   "source": [
    "sn.set_theme(style=\"whitegrid\")\n",
    "plot_data_XGB(y_valid,y_preds_XGB,\"Validation data XGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZThHw7Bhnr6H"
   },
   "outputs": [],
   "source": [
    "# Standard scaler - transform\n",
    "x_scaled_test = scaler.transform(data_dl_descriptors)\n",
    "\n",
    "# Predict solubility of the test data\n",
    "y_test_preds_XGB = model_XGB.predict(x_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZF0BS7TqxVE"
   },
   "outputs": [],
   "source": [
    "# Plotting testing set for XGB\n",
    "sn.set_theme(style=\"whitegrid\")\n",
    "plot_data_XGB(data_dl[\"LogS exp (mol/L)\"], y_test_preds_XGB,\n",
    "           \"Test data XGB: Drug-like Molecules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMngOOWyYomD"
   },
   "source": [
    "## 7.3 Random Forrest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zysukbYWYvTN"
   },
   "source": [
    "Let's test the Random Forest model with optimized values we obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "x28QGxpMYurY"
   },
   "outputs": [],
   "source": [
    "# Replace NaN values by the average\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Imput the misssing values in x_train_scaled\n",
    "x_train_imputed = imputer.fit_transform(x_train_scaled)\n",
    "\n",
    "# Model creation for RandomForestRegressor\n",
    "modelRF = RandomForestRegressor(n_estimators=901,\n",
    "                               max_depth=26,\n",
    "                               random_state=42)\n",
    "\n",
    "# Model training on the given data\n",
    "modelRF.fit(x_train_imputed, y_train)\n",
    "\n",
    "# Imput missing values in x_valid_scaled\n",
    "x_valid_imputed = imputer.transform(x_valid_scaled)\n",
    "\n",
    "# Prediction\n",
    "y_preds_RF = modelRF.predict(x_valid_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "bkI1IpeAhBAJ"
   },
   "outputs": [],
   "source": [
    "# A plotting function\n",
    "def plot_data_RF(actual, predicted, title):\n",
    "\n",
    "# model performance using RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# R^2 (coefficient of determination) :\n",
    "    R2 =r2_score(actual, predicted)\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot the figure\n",
    "    sn.regplot(x=predicted , y=actual,line_kws={\"lw\":2,\n",
    "                                                \"ls\":\"--\",\n",
    "                                                \"color\":\"red\",\n",
    "                                                \"alpha\":0.7})\n",
    "    plt.title(title, color=\"red\")\n",
    "    plt.xlabel(\"Predicted logS(mol/L)\",\n",
    "               color=\"blue\")\n",
    "    plt.xlim(-8,1)\n",
    "    plt.ylabel(\"Experimental logS(mol/L)\",\n",
    "               color =\"blue\")\n",
    "\n",
    "\n",
    "    plt.grid(alpha=0.3)\n",
    "    R2 = mpatches.Patch(label=\"R2={:04.2f}\".format(R2))\n",
    "    rmse = mpatches.Patch(label=\"RMSE={:04.2f}\".format(rmse))\n",
    "    plt.legend(handles=[R2, rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "x4SrRw_Ciz8H"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Calculate molecular descriptors for the test data\n",
    "#Mol_descriptors_test, desc_names_test = RDkit_descriptors(data_dl[\"SMILES\"])\n",
    "#data_dl_descriptors = pd.DataFrame(Mol_descriptors_test, columns=desc_names_test)\n",
    "\n",
    "# Impute missing values in the test data\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "x_scaled_test_imputed = imputer.fit_transform(data_dl_descriptors)\n",
    "\n",
    "# Check the number of features in the training data (assuming it's the same as the model's expected features)\n",
    "num_train_features = x_train_scaled.shape[1]\n",
    "\n",
    "# If the number of features in the test data does not match the number of features in the training data,\n",
    "# adjust the test data accordingly\n",
    "if x_scaled_test_imputed.shape[1] != num_train_features:\n",
    "    if x_scaled_test_imputed.shape[1] < num_train_features:\n",
    "        # Add dummy columns for missing features\n",
    "        num_missing_features = num_train_features - x_scaled_test_imputed.shape[1]\n",
    "        dummy_columns = np.zeros((x_scaled_test_imputed.shape[0], num_missing_features))\n",
    "        x_scaled_test_adjusted = np.hstack((x_scaled_test_imputed, dummy_columns))\n",
    "    elif x_scaled_test_imputed.shape[1] > num_train_features:\n",
    "        # Remove excess features\n",
    "        x_scaled_test_adjusted = x_scaled_test_imputed[:, :num_train_features]\n",
    "else:\n",
    "    x_scaled_test_adjusted = x_scaled_test_imputed\n",
    "\n",
    "# Predict solubility of the adjusted test data\n",
    "y_test_preds = modelRF.predict(x_scaled_test_adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhzLJ-SyR-ru"
   },
   "source": [
    "Let's plot the predicted logS of the validation set and see if our model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4RdYWfroasd"
   },
   "outputs": [],
   "source": [
    "# Plotting testing set\n",
    "sn.set_theme(style=\"whitegrid\")\n",
    "plot_data_RF(data_dl[\"LogS exp (mol/L)\"], y_test_preds,\n",
    "           \"Test data: Drug-like Molecules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONSdyrGvo80R"
   },
   "outputs": [],
   "source": [
    "# Standard scaler - transform\n",
    "x_scaled_test = scaler.transform(data_dl_descriptors)\n",
    "\n",
    "# Predict solubility of the test data\n",
    "y_test_preds = modelRF.predict(x_scaled_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG-nNTVGJGQJ"
   },
   "source": [
    "#8. Saving of the trained model and standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBUk2hL7JdwC"
   },
   "outputs": [],
   "source": [
    "#Save the model and the scaler using pickel\n",
    "import pickle\n",
    "with open(\"model_LGBM.pkl\",\"wb\") as f:\n",
    "    pickle.dump(model,f)\n",
    "\n",
    "with open(\"scaler_LGBM.pkl\",\"wb\") as f:\n",
    "    pickle.dump(custom_scaler,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBtgUDV_aU54"
   },
   "source": [
    "#9. Using the .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yan4kBMnaeeu"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import model # Import your specific model\n",
    "\n",
    "# Load the pickle file\n",
    "with open('your_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
